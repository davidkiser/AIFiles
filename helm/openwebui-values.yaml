# Open WebUI Helm values (RTX 3090 + Ryzen 7 5800X, tuned for single-user)

nameOverride: ""
namespaceOverride: ""

# ----- OLLAMA (GPU) -----
ollama:
  enabled: true
  fullnameOverride: "open-webui-ollama"

  image:
    tag: "0.11.10" 

  persistentVolume:
    enabled: true
    existingClaim: ai-data

  # Keep your original gpu block (chart may use it)
  gpu:
    enabled: true
    type: nvidia
    number: 1

  runtimeClassName: nvidia

  # ADD: explicit GPU request so K8s assigns a GPU
  resources:
    requests:
      cpu: "4"
      memory: "12Gi"
    limits:
      cpu: "8"
      memory: "60Gi"
      nvidia.com/gpu: 1      # <— added

  # Keep original envs
  extraEnvVars:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility"
    - name: OLLAMA_NUM_PARALLEL
      value: "1"
    - name: OLLAMA_MAX_LOADED_MODELS
      value: "1"
    - name: OLLAMA_KEEP_ALIVE
      value: "10m"
    - name: OLLAMA_FLASH_ATTENTION
      value: "1"
    - name: OLLAMA_LLM_LIBRARY
      value: "cuda"
    - name: OLLAMA_NUM_GPU
      value: "1"
    - name: OLLAMA_HOST
      value: "0.0.0.0"
    - name: OLLAMA_PORT
      value: "11434"

  # Keep your label so the Service selects this pod
  podLabels:
    app.k8s.guru/ollama: "true"

  # Models (unchanged)
  models:
    pull:
      - llama4:maverick-q4_K_M
    run:
      - llama4:maverick-q4_K_M

# ----- Pipelines -----
pipelines:
  enabled: true
  extraEnvVars: []
  resources:
    requests:
      cpu: "1"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "2Gi"

# ----- Apache Tika -----
tika:
  enabled: true

# External Ollama endpoints (not used here)
ollamaUrls: []
ollamaUrlsFromExtraEnv: false

# ----- WebSocket (with lightweight Redis) -----
websocket:
  enabled: true
  manager: redis
  url: redis://open-webui-redis:6379/0
  nodeSelector: {}
  redis:
    enabled: true
    name: open-webui-redis
    image:
      repository: redis
      tag: 7.4.2-alpine3.21
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    service:
      containerPort: 6379
      type: ClusterIP
      port: 6379

# No Redis cluster for single-user
redis-cluster:
  enabled: false

# Cluster domain
clusterDomain: cluster.local

# ----- Core Open WebUI Deployment -----
replicaCount: 1

image:
  repository: ghcr.io/open-webui/open-webui
  tag: ""           # keep chart default (AppVersion)
  pullPolicy: IfNotPresent

serviceAccount:
  enable: true
  name: ""
  automountServiceAccountToken: false

imagePullSecrets: []

# Health probes
livenessProbe:
  httpGet:
    path: /health
    port: http
  failureThreshold: 3
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /health/db
    port: http
  failureThreshold: 3
  periodSeconds: 10

# Open WebUI app container resources
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "2"
    memory: "4Gi"

# ----- Managed Certificate (disabled) -----
managedCertificate:
  enabled: false
  name: "mydomain-chat-cert"
  domains:
    - chat.example.com

# ----- Persistence for Open WebUI state -----
persistence:
  enabled: true
  size: 2Gi
  accessModes: [ReadWriteOnce]
  provider: local

# ----- Service exposure (Open WebUI app, not Ollama) -----
service:
  type: NodePort
  port: 80
  containerPort: 8080
  nodePort: 30080

# Enable OpenAI API proxying (optional)
enableOpenaiApi: true
openaiBaseApiUrl: "https://api.openai.com/v1"

# Keep EXACTLY your original in-cluster URL
extraEnvVars:
  - name: OLLAMA_BASE_URL
    value: "http://ollama-svc.ai.svc.cluster.local:11434"

commonEnvVars: []
extraEnvFrom: []

# ----- SSO (disabled for single-user) -----
sso:
  enabled: false

# ----- Database (Postgres) — disabled -----
postgresql:
  enabled: false

# Logging
logging:
  level: "debug"

# ----- Extra objects: NodePort Service for Ollama 11434→31434 (unchanged)
extraObjects:
  - apiVersion: v1
    kind: Service
    metadata:
      name: ollama-svc
      labels:
        app.kubernetes.io/component: ollama
    spec:
      type: NodePort
      selector:
        app.k8s.guru/ollama: "true"
      ports:
        - name: http
          port: 11434
          targetPort: 11434
          nodePort: 31434
